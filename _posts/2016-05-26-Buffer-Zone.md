---
published: true
layout: post
---
I have been busy lately, and a lot of great papers have been crawling on my brower's tab space. I can't put them into my paper collection because I haven't read them, but I can't just close the tabs and forget about them. This is a temporary buffer zone for papers that I want to read but didn't have time.

Dynamic Memory Network Implementation: [http://yerevann.github.io/2016/02/23/playground-for-babi-tasks/](http://yerevann.github.io/2016/02/23/playground-for-babi-tasks/) and [http://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks/#next-steps](http://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks/#next-steps)
(Actual Git repo: [https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano](https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano))


Stochastic Variance Reduction for Nonconvex optimization: [http://suvrit.de/papers/nonconvex_svrg.pdf](http://suvrit.de/papers/nonconvex_svrg.pdf)

ICML Workshop on Non-convex optimization (Percy Liang being an organizer): [https://sites.google.com/site/noncvxicml16/](https://sites.google.com/site/noncvxicml16/)

Quoc Le's Neural Programmer - embedding arithmetic computation into neural network: [http://arxiv.org/pdf/1511.04834v2.pdf](http://arxiv.org/pdf/1511.04834v2.pdf)

Quoc Le, Luke Vilnis: introducing Gaussian gradient noise to improve deep network learning [http://arxiv.org/pdf/1511.06807v1.pdf](http://arxiv.org/pdf/1511.06807v1.pdf)

Jiwei's diversity promoting conversational model, where it makes sense to impose not only P(Target|Source) but also P(Source|Target) - non-conventional loss function: [http://arxiv.org/pdf/1510.03055v2.pdf](http://arxiv.org/pdf/1510.03055v2.pdf)

Jiwei's reommendation, also latest paper on using reinforcement learning to incoroporate non-conventional loss functions (such as train on BLEU score that can't be worked into the cost in a normal way): [http://arxiv.org/pdf/1511.06732v7.pdf](http://arxiv.org/pdf/1511.06732v7.pdf)

Amazing article on how to view Residual Network as an ensemble: [http://arxiv.org/pdf/1605.06431v1.pdf](http://arxiv.org/pdf/1605.06431v1.pdf)

Incorporating Loose-Structured Knowledge into LSTM with Recall Gate
for Conversation Modeling: [https://arxiv.org/pdf/1605.05110v1.pdf](https://arxiv.org/pdf/1605.05110v1.pdf)

Laurens van der Maaten  who invented TsNE: Learning with Marginalized Corrupted Features - introducing Gaussian, Laplacian, Poisson, or Blankout noise  [http://jmlr.csail.mit.edu/proceedings/papers/v28/vandermaaten13.pdf](http://jmlr.csail.mit.edu/proceedings/papers/v28/vandermaaten13.pdf)

Tensorflow also has a scan module! This is a tutorial on how to use it: [http://rdipietro.github.io/tensorflow-scan-examples/](http://rdipietro.github.io/tensorflow-scan-examples/)