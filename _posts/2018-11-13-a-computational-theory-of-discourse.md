---
published: true
layout: post
draft: true
visible: 0
title: A Computational Theory of Discourse
---
## A Computational Theory of Discourse

Note: to people who are infinitely more well-versed in linguistics than I am, here I use "discourse" to indicate the meaning representation of a sentence. 

TLDR: This is a post based on Linear Algebraic Structure of Word Senses, with Applications to Polysemy ([link](https://arxiv.org/pdf/1601.03764.pdf)).

### Short Preface

There has been a lot interests on sentence-level representation learning. Similar to the explosion of word embedding, sentence embedding has captured a lot of attention with new papers being produced on ArXiv like grass sprouting in Spring. 

This article could easily be a lengthy linguistics paper, and such one is very needed for the current situation. Based on my very limited education in linguistics, I do not believe linguistis have an agreed upon set of "properties" that a sentence representation[^1] need to capture, nor do I believe adding more of such properties exhausitvely will finally have enough granularity to sift gold from sand -- find the most ideal sentence representation.

This post tries to take a closer look at the seminal work produced by Sanjeev Arora and his students, who took a principled approach to computationally infer the meaning representation from words, and then to sentences.

### A Generative Model of Sentence

<img src="https://github.com/windweller/windweller.github.io/blob/master/images/discourse-graph.png?raw=true" style="width:50%">

Arora et al. assume a discourse random vector[^2]  $c \sim \mathcal{N}(0, \Sigma)$ that represents the meaning of a sentence,  then every word of the sentence is generated by this random variable $c$. The probabilistic graphical representation is illustrated above. Then we can write the overall model as words in a window of size $n$ generated by $c$ as:
$$
P(w_1, w_2, ..., w_n | c) = \prod_{i=1}^n P(w_i | c)
$$
The probability of a word $w_i$ occur in the sequence can be described as a log-linear model:
$$
P(w_i | c) = \frac{\exp(c \cdot v_{w_i})}{Z_c}
$$
Note that $v_{w_i}$ indicates the actual vector representation of $w_i$, and this probability model uses dot-product distance to capture the probability of word $w_i$ appear in the sequence, normalized by a partition function $Z_c = \sum_w \exp(v_w \cdot c)$. This dictates that the discourse vector $c$ will be close to words it produces in vector space.

Based on this very simple model, a very interesting theorem can be proved. Here, I write out the actual proof (with more details than the one provided in the paper, as well as an easy illustration on what's going on).

Theorem 1 of Arora et al. (2018)[^3] paper can be understood by introducing a vector $u$, and a matrix $A$.  For a given word $w$ and its corresponding word vector $v_w$, we can compute a vector $u$. 

<img src="https://github.com/windweller/windweller.github.io/blob/master/images/discourse-theorem1-u.jpg?raw=true" width=35%>

For this word $w$, it must appear in different spans of words across the entire document. A random variable of a window of n words can be introduced as $s$. Computationally, the vector $u$ for the word $w$ can be computed as follow:
$$
u = \frac{1}{k} \sum_{s \in \{s_1, ..., s_k\}} \frac{1}{n} \sum_{w_i \in s} v_{w_i}
$$
To even make this statement simpler, assume the above figure represents a tensor $S \in \mathcal{R}^{n \times k \times |v_{w_i}|}$, we can easily run the following Tensorflow operation to obtain $u$: `u = tf.reduce_mean(tf.reduce_mean(S, axis=0), axis=1)`. After knowing how $u$ is computed, then we can understand Theorem 1:
$$
v_w = A u
$$
For any word, if we compute the corresponding vector $u$, the word embedding of this word can be obtained through a linear transformation (matrix multiplication) by a fixed matrix $A$. 

### Application to Word Senses

### Relations to Language Modeling

[^1]: In the scope of this post, we can assume it's an embedding. This is a very narrow interpretation that is ignoring decades of linguistic work on sentence representations. Interested readers can take a look at Kemp's Discourse Representation Theory framework.
[^2]: In most of Arora et al.'s work, "sentence meaning", "discourse", and "context" are used almost interchangeably. They all refer to a vector representation of a span of words, usually within a fixed window.
[^3]:Linear Algebraic Structure of Word Senses, with Applications to Polysemy.
