---
published: true
layout: post
draft: true
visible: 0
title: A Generative Model of Discourse
---
## A Generative Model of Discourse

Note: to people who are infinitely more well-versed in linguistics than I am, here I use "discourse" to indicate the meaning representation of a sentence. 

TLDR: This is a post based on Linear Algebraic Structure of Word Senses, with Applications to Polysemy ([link](https://arxiv.org/pdf/1601.03764.pdf)).

### Short Preface

There has been a lot interests on sentence-level representation learning. Similar to the explosion of word embedding, sentence embedding has captured a lot of attention with new papers being produced on ArXiv like grass sprouting in Spring. 

This article could easily be a lengthy linguistics paper, and such one is very needed for the current situation. Based on my very limited education in linguistics, I do not believe linguistis have an agreed upon set of "properties" that a sentence representation[^1] need to capture, nor do I believe adding more of such properties exhausitvely will finally have enough granularity to sift gold from sand -- find the most ideal sentence representation.

This post tries to take a closer look at the seminal work produced by Sanjeev Arora and his students, who took a principled approach to computationally infer the meaning representation from words, and then to sentences.

### A Generative Model of Sentence

<p style="text-align: center"><img src="https://github.com/windweller/windweller.github.io/blob/master/images/discourse-graph.png?raw=true" style="width:50%"> <br> <b> Figure 1 </b></p>

Arora et al. assume a discourse random vector[^2]  $c \sim \mathcal{N}(0, \Sigma)$ that represents the meaning of a sentence,  then every word of the sentence is generated by this random variable $c$. The probabilistic graphical representation is illustrated above. Then we can write the overall model as words in a window of size $n$ generated by $c$ as:

$$
P(w_1, w_2, ..., w_n | c) = \prod_{i=1}^n P(w_i | c)
$$

The probability of a word $w_i$ occur in the sequence can be described as a log-linear model:

$$
P(w_i | c) = \frac{\exp(c \cdot v_{w_i})}{Z_c}
$$

Note that $v_{w_i}​$ indicates the actual vector representation of $w_i​$, and this probability model uses dot-product distance to capture the probability of word $w_i​$ appear in the sequence, normalized by a partition function $Z_c = \sum_w \exp(v_w \cdot c)​$. This dictates that the discourse vector $c​$ will be close to words it produces in vector space.

Based on this very simple model, a very interesting theorem can be proved. Here, I write out the actual proof (with more details than the one provided in the paper, as well as an easy illustration on what's going on).

Theorem 1 of Arora et al. (2018)[^3] paper can be understood by introducing a vector $u$.  For a given word $w$ and its corresponding word vector $v_w$, we can compute a vector $u$. 

<p style="text-align: center"><img src="https://github.com/windweller/windweller.github.io/blob/master/images/discourse-theorem1-u.jpg?raw=true" style="width:35%"> <br> <b>Figure 2</b> </p>

For this word $w$, it must appear in different spans of words across the entire document. A random variable of a window of $n$ words can be introduced as $s$, a span. Computationally, the vector $u$ for the word $w​$ can be computed as follow:

$$
u = \frac{1}{k} \sum_{s \in \{s_1, ..., s_k\}} \frac{1}{n} \sum_{w_i \in s} v_{w_i}
$$

To even make this statement simpler, assume the above figure represents a tensor $S \in \mathcal{R}^{n \times k \times d}​$, we can easily run the following Numpy operation to obtain $u​$: `u = np.mean(np.mean(S, axis=0), axis=1)`. After knowing how $u​$ is computed, then we can understand Theorem 1:
$$
v_w = A u
$$

For any word, if we compute the corresponding vector $u$, the word embedding of this word can be obtained through a linear transformation (matrix multiplication) by a fixed matrix $A$. I provide some algebra walk through the proof of Theorem 1 in the paper. Readers who find it elementary or advanced can skip this block straight to the next section. 

(**Optional**)

The proof stands as long as the generative model in Figure 1 holds. We set to show that $\mathbb{E}(c_s | w \in s) = \mathbb{E}(\frac{1}{n} \sum_{w_i \in s} v\_{w_i} \vert w \in s)$. By "iterated expectation" or "law of total expectation", we can expand the left-hand side (LHS) as:
$$
\begin{equation}
\mathbb{E}[c_s | w \in s] = \mathbb{E}[\mathbb{E}[c_s | s = w_1...w...w_n | w \in s]]
\end{equation}
$$

The following step is to find the probability density function (pdf) of $c \vert w$: $p(c \vert w)$. In the earlier portion of the paper, we have the following equalities that we can substitute: $Z\_c \approx Z \exp(\vert\vert c \vert\vert^2)$[^4], the probability density function of a multivariate normal distribution $c \sim (0, \Sigma)$ is $p(c) = \exp(-\frac{1}{2} c^T \Sigma^{-1}c)$, $\vert\vert c \vert\vert^2 = c^Tc = c^T I c$, and the log-linear model we assumed: $p(w \vert c) = \exp(c \cdot v\_w)$. We can expand $p(c\vert w)​$ using Bayes rule and substitute these terms in and obtain:

$$
\begin{align*}
p(c|w) &\propto p(w|c)p(c) \\
&\propto \frac{1}{Z} \exp(v_w \cdot c - c^T(\frac{1}{2} \Sigma^{-1} + I)c) \\
\end{align*}
$$

After obtaining the probability density function of $c \vert w​$, we can think about what kind of random variable this pdf suggests, because eventually we want to know what is $\mathbb{E}(c \vert w)​$, the left hand side of equation (1). Since there is a covariance matrix inverse $\Sigma^{-1}​$ invovled, we can try to re-arrange the terms to make it look more like a multivariate Gaussian distribution. Since we do want to know $\mathbb{E}(c \vert w)​$, we need to know what is the mean of this new distribution.

First, we ignore the covariance determinant term as it is a constant and in Arora's setting, the covariance matrix is invertible -- "if the word vectors as random variables are isotropic to some extent, it will make the covariance matrix identifiable" (identifiable = invertible). The assumption "isotropic word embedding" here means that word embedding dimensions should not be correlated with each other.

Then, all we need to do is to rearrange the terms in $p(c \vert w)​$ to appear in the form of $\exp(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu))​$. By doing so, we will be able to find our $\mu​$, the expectation of this pdf. Since the form $c^T(\frac{1}{2} \Sigma^{-1} + I)c​$ looks very similar to the quadratic form that we need, we can let $B^{-1} = \frac{1}{2} \Sigma^{-1} + I​$ and let $B​$ be our new covariance matrix for $c \vert w​$. We can work out the equations from both sides. We let $\mu​$ be the mean we want and solve for it:

$$
\begin{align*}
p(c|w) &\propto \exp(-\frac{1}{2} (c-\mu)^T B^{-1} (c-\mu)) \\
&= \exp(-\frac{1}{2}(c^T B^{-1} c - cB^{-1}\mu - \mu^TB^{-1}c + \mu^TB^{-1}\mu))\\
p(c|w) &\propto \frac{1}{Z} \exp(v_w \cdot c - c^T(\frac{1}{2} \Sigma^{-1} + I)c) \\
&= \frac{1}{Z} \exp(-\frac{1}{2}(-2 v_w \cdot c + c^TB^{-1}c))
\end{align*}
$$

Now we have two expressions of $p(c \vert w)$. We can match the terms between two equations, one term $c^TB^{-1}c$ already appears in both, but not $-2 v_w \cdot c$. However, there are two terms with negative signs in the top expansion. A trick that applies here is to just make them equal and hope things to work out -- we solve for $\mu$:

$$
-2 v_w \cdot c = - cB^{-1}\mu - \mu^TB^{-1}c
$$

It is somewhat transparent that on the RHS (right hand side), $B$ needs to disappear since the LHS (left hand side) does not contain any $B$. To do that, $\mu$ should at least contain $B$ so that it cancels out with $B^{-1}$. Also the LHS has $v_w$ while RHS has none. Then the answer should be apparent: $\mu = Bv_w$. If you plug this in, the above equality works, shows that this is our $\mu​$. 

My stats PhD friend told me, if I saw a pdf in the form of $w^Tx - \frac{1}{2} x^TB^{-1}x​$, then I can actually skip the above algebra and directly "see" this distribution of $x​$ as mean $Bw​$, with variance $B​$. 

So now, we know that $c \vert w \sim \mathcal{N}(B^{-1}v_w, B)​$ where $B = (\Sigma^{-1} + 2I)^{-1}​$, the posterior distribution of $c​$ after conditioning on a single word in the sequence. Thus  $\mathbb{E}(c \vert w) = (\Sigma^{-1} + 2I)^{-1} v_w​$.

<p>Then we want to get the pdf that describes $c|w_1, ..., w_n​$. This part is relatively straightforward, no algebra trick / insight is required. The work mostly hinges on the following expression: </p>

$$
p(c|w_1, ..., w_n) \propto p(w_1,...,w_n|c) p(c) \propto p(c) \prod_{i=1}^n p(w_i|c) \\
= \frac{1}{Z^n} \exp(\sum_{i=1}^n v_{w_i}^Tc - \frac{1}{2} c^T(\Sigma^{-1} + 2nI)c)
$$

<p>The generation of words are independent with each other conditioned on $c​$. We already know the expression of $p(w \vert c)​$. So the above equation evaluates to a form that we have already worked out before. We can skip the algebra and know that $\mathbb{E}[c \vert w_1, ..., w_n] \approx (\Sigma^{-1} + 2nI)^{-1} \sum_{i=1}^n v\_{w_i}​$.</p>

If you still recall the LHS and RHS of the Equation (1), then what we have left to conclude the proof is to plug  what we have derived into the LHS and RHS. Feel free to refer to the paper since it offers a cleaner/shorter presentation.

$$
\mathbb{E}[c_s | w \in s] = \mathbb{E}[\mathbb{E}[c_s | s = w_1...w...w_n | w \in s]] \\
 (\Sigma^{-1} + 2I)^{-1} v_w \approx (\Sigma^{-1} + 2nI)^{-1} \sum_{i=1}^n v_{w_i}
$$

Therefore, we know that the matrix $A$ that we set out to find is now solvable by re-arranging the terms in above equations: $A = n(\Sigma^{-1} + 2I) (\Sigma^{-1} + 2nI)^{-1}​$.

### Finding Linear Transformation

If we suppose that $u$ is the averaged discourse vectors, then iterating through the vocabulary, we should be able to find matrix $A$. 

### Application to Word Senses

Intuitively, Theorem 1 dictates that a word has a **linear relationship** (fulfilled by matrix $A​$) to the average of all the context vectors this word appears in: $v\_w = A \sum_s c_s​$. This relationship is fully specified by $\Sigma​$, the covariance of discourse random vector $c​$. Theorem 2 of the paper can be interpreted as: $v\_w \approx \alpha v\_{s\_1} + \beta v\_{s\_2}​$, a linear combination of 2 senses (each sense is represented as a vector). We can see the parallel between this linear decomposition and the transformed average of all context that word $w​$ appears in. The proof of Theorem 2 essentially falls in this line, if there are 2 senses for a given word, then with high probability, $\alpha​$% of the context vectors should be similar to each other as they are all this word expressed in sense 1, $\beta​$% of the context vectors should be similar/grouped together as they are all expressed as sense 2.

Since we do not observe the frequency ($\alpha$, $\beta$), nor do we know how many senses are in there, Arora proposed to discover senses using **sparse coding**[^5], finding a set of unit vectors $A\_1, ...,A\_m$, that for any word $v\_w$, it can be expressed by a small number of these unit vectors. These unit vectors are referred as the **Atoms of Discourse**.

Sparse coding objective and description can be found in the paper, overall, given a set of word vectors, two integers k, m with k << m, find a set of unit vectors $A\_1, ...,A\_m$ such that:

$$
v_w = \sum_{j=1}^m \alpha_{w,j}A_j + \eta_w
$$

where at most k of the coefficients $\alpha$ are nonzero.  The goal is to minimize the reconstruction error term $\sum\_w \eta\_w$.

$$
\sum_w ||v_w - \sum_{j=1}^m \alpha_{w,j} A_j||_2^2
$$

### Relations to Language Modeling



### Implicit Solutions 

Unwittingly at first, Word2Vec is quickly shown to be an implicit solution to a non-convex co-occurence matrix decomposition. GloVE and other word embedding methods followed the lead and grounded these methods in theory. Are sentence embedding models, let it be InferSent, DisSent, ELMo, BERT, implicit solutions to a more principled discourse model? 

[^1]: In the scope of this post, we can assume it's an embedding. This is a very narrow interpretation that is ignoring decades of linguistic work on sentence representations. Interested readers can take a look at Kemp's Discourse Representation Theory framework.
[^2]: In most of Arora et al.'s work, "sentence meaning", "discourse", and "context" are used almost interchangeably. They all refer to a vector representation of a span of words, usually within a fixed window. 
[^3]:Linear Algebraic Structure of Word Senses, with Applications to Polysemy.
[^4]: This is proven in A Latent Variable Model Approach to PMI-based Word Embeddings, Lemma 2.1. They proved a concentration bound of this partition function under the Bayesian priors specified in the model of Figure 1. It seems to be a general bound linked to the self-normalizing property of log-linear models.

[^5]: http://ufldl.stanford.edu/wiki/index.php/Sparse_Coding

