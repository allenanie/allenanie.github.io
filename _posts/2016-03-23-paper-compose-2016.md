---
published: true
title: 2016 Paper Selection
layout: post
---


I decide to have a series of introductions for cutting edge papers. Those papaers are posted on Archive site like [arxiv.org](http://arxiv.org/). I recently subscribed to them. I will list those articles based on themes, or relevancy, or (fancier) timelines. I won't discuss in great detail the content of the paper, but I'll leave my comments.

## 2016 March

March is actually a great month, mostly due to the fact that ACL is finally wrapped up and we are seeing more papers being submitted. 

### Miscellaneous 

- **Topic Modeling Using Distributed Word Embeddings**

   by Ramandeep S Randhawa, Parag Jain, Gagan Madan
   
   University of Southern California, IIT Delhi
   
   Abstract: We propose a new algorithm for topic modeling, Vec2Topic, that identifies the main topics in a corpus using semantic information captured via high-dimensional distributed word embeddings. Our technique is unsupervised and generates a list of topics ranked with respect to importance. We find that it works better than existing topic modeling techniques such as Latent Dirichlet Allocation for identifying key topics in user-generated content, such as emails, chats, etc., where topics are diffused across the corpus. We also find that Vec2Topic works equally well for non-user generated content, such as papers, reports, etc., and for small corpora such as a single-document.
   
   link: [http://arxiv.org/abs/1603.04747](http://arxiv.org/abs/1603.04747)
   
   Comments: Might be interesting to check out, a replacement for LDA
   
- **The Computational Power of Dynamic Bayesian Networks**

   by Joshua Brulé
   
   University of Maryland
   
   Abstract: This paper considers the computational power of constant size, dynamic Bayesian networks. Although discrete dynamic Bayesian networks are no more powerful than hidden Markov models, dynamic Bayesian networks with continuous random variables and discrete children of continuous parents are capable of performing Turing-complete computation. With modified versions of existing algorithms for belief propagation, such a simulation can be carried out in real time. This result suggests that dynamic Bayesian networks may be more powerful than previously considered. Relationships to causal models and recurrent neural networks are also discussed.
   
   link: [http://arxiv.org/abs/1603.06125](http://arxiv.org/abs/1603.06125)
   
   Comments: Dynamic Bayesian network is Turing-complete. Maybe it can give neural network a run for its money??

### Computer Vision

- **Object Contour Detection with a Fully Convolutional Encoder-Decoder Network**

   by Jimei Yang, Brian Price, Scott Cohen, Honglak Lee, Ming-Hsuan Yang
   
   Adobe Research
   
   Abstract: We develop a deep learning algorithm for contour detection with a fully convolutional encoder-decoder network. Different from previous low-level edge detection, our algorithm focuses on detecting higher-level object contours. Our network is trained end-to-end on PASCAL VOC with refined ground truth from inaccurate polygon annotations, yielding much higher precision in object contour detection than previous methods. We find that the learned model generalizes well to unseen object classes from the same super-categories on MS COCO and can match state-of-the-art edge detection on BSDS500 with fine-tuning. By combining with the multiscale combinatorial grouping algorithm, our method can generate high-quality segmented object proposals, which significantly advance the state-of-the-art on PASCAL VOC (improving average recall from 0.62 to 0.67) with a relatively small amount of candidates (∼1660 per image).
   
   link: [http://arxiv.org/abs/1603.04530](http://arxiv.org/abs/1603.04530)
   
   Comments: We have seen RNN encoder-decoder network, but what about CNN encoder-decoder? This might provide some answer. The encoder part is a normal CNN to condense the spatial information, the decoder part is a "deconv" network.

- ** (Image) Segmentation from Natural Language Expressions**

   by Ronghang Hu, Marcus Rohrbach, Trevor Darrell
   
   Berkeley
   
   Abstract: In this paper we approach the novel problem of segmenting an image based on a natural language expression. This is different from traditional semantic segmentation over a predefined set of semantic classes, as e.g., the phrase "two men sitting on the right bench" requires segmenting only the two people on the right bench and no one standing or sitting on another bench. Previous approaches suitable for this task were limited to a fixed set of categories and/or rectangular regions. To produce pixelwise segmentation for the language expression, we propose an end-to-end trainable recurrent and convolutional network model that jointly learns to process visual and linguistic information. In our model, a recurrent LSTM network is used to encode the referential expression into a vector representation, and a fully convolutional network is used to a extract a spatial feature map from the image and output a spatial response map for the target object. We demonstrate on a benchmark dataset that our model can produce quality segmentation output from the natural language expression, and outperforms baseline methods by a large margin.
   
   link: [http://arxiv.org/abs/1603.06180](http://arxiv.org/abs/1603.06180)
   
   Comments: Very interesting article. A little bit too long. It could be useful for any kind of image-mixed-with-language application (such as AR or VR).
   

### NLP

- **End-to-End Attention-based Large Vocabulary Speech Recognition**

   by Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, Yoshua Bengio
   
   University of Montreal, Bengio Group
   
   Abstract: Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.
   
   link: [http://arxiv.org/abs/1508.04395](http://arxiv.org/abs/1508.04395)
   
   Comments: This is a natural extension to what Bahdanau worked on in attention-based neural machine translation (he is the one who came up with attention: [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](http://arxiv.org/pdf/1409.0473v6.pdf)). Now they are applying the model to speech recognition.
   
- **Tree-to-Sequence Attentional Neural Machine Translation**

   by Akiko Eriguchi, Kazuma Hashimoto, Yoshimasa Tsuruoka
   
   The University of Tokyo
   
   Abstract: Most of the existing neural machine translation (NMT) models focus on the conversion of sequential data and do not directly take syntax into consideration. We propose a novel end-to-end syntactic NMT model, extending a sequence-to-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT'15 English-to-Japanese dataset demonstrate that our proposed model outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system.
   
   link: [http://arxiv.org/abs/1603.06075](http://arxiv.org/abs/1603.06075)
   
   Comments: I think there are two camps of the deep learning NLP right now. One camp (mostly from the big three Deep Learning groups) is trying to be agnostic to linguistic knowledge. They are developing character-based models to learn language from ground up. Another camp is to try their best to incorporate linguistic structure or knowledge into deep learning framework. This paper is a beautiful example of the latter. It shows how people the proper way to bake syntax into neural network models.
   
- **How Transferable are Neural Networks in NLP Applications?**

   by Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, Zhi Jin
   
   Peking University
   
   Abstract: Transfer learning is aimed to make use of valuable knowledge in a source domain to help the model performance in a target domain. It is particularly important to neural networks because neural models are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct a series of empirical studies and provide an illuminating picture on the transferability of neural networks in NLP.

   
   link: [http://arxiv.org/abs/1603.06111](http://arxiv.org/abs/1603.06111)
   
   Comments: This article is building from the transfer learning literature in Computer Vision arena. I skipped the paper and read the conclusion and it turns out to be related to how similar two corpuses or two genres are. Seems to be a bit platitudinous.

- **A Character-level Decoder without Explicit Segmentation for Neural Machine Translation**

   by Junyoung Chung, Kyunghyun Cho, Yoshua Bengio
   
   University of Montreal, New York University
   
   Abstract: The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.
   
   link: [http://arxiv.org/abs/1603.06147](http://arxiv.org/abs/1603.06147)
   
   Comments: Take it from the University of Tokyo's tree-based article, this article is running off to the end of the other spectrum. For this paper, they only investigated generating target sentence character-by-character, but they suggest future literature to investigate reading in source sentences character-by-character. (They did slightly better than state-of-the-art with an ensemble model)
   
- **A Persona-Based Neural Conversation Model**

   by Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan
   
   Stanford University (Jurafsky Group), Microsoft Research
   
   Abstract: We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gain in speaker consistency as measured by human judges.
   
   link: [http://arxiv.org/abs/1603.06155](http://arxiv.org/abs/1603.06155)
   
   Comments: (First of all, I talked with Jianfeng Gao at NAACL 2015 lol, glad to see his name here) It's just a fun paper to read. The proposed model is straight-forward and easy to understand. It could be applied to multiple things.
   
- **Harnessing Deep Neural Networks with Logic Rules**

   by Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, Eric Xing
   
   Carnegie Mellon University
   
   Abstract: Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce unpredictability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.
   
   link: [http://arxiv.org/abs/1603.06318](http://arxiv.org/abs/1603.06318)
   
   Comments: Logic is the backbone of noun-based semantics analysis, and many other things. Baking logic rules into the proposed teacher-student model could be really cool.
   
- **Stack-propagation: Improved Representation Learning for Syntax**

   by Yuan Zhang, David Weiss
   
   MIT, Google
   
   Abstract: Traditional syntax models typically leverage part-of-speech (POS) information by constructing features from hand-tuned templates. We demonstrate that a better approach is to utilize POS tags as a regularizer of learned representations. We propose a simple method for learning a stacked pipeline of models which we call "stack-propagation". We apply this to dependency parsing and tagging, where we use the hidden layer of the tagger network as a representation of the input tokens for the parser. At test time, our parser does not require predicted POS tags. On 19 languages from the Universal Dependencies, our method is 1.3% (absolute) more accurate than a state-of-the-art graph-based approach and 2.7% more accurate than the most comparable greedy model.
   
   link: [http://arxiv.org/pdf/1603.06598v1.pdf](http://arxiv.org/pdf/1603.06598v1.pdf)
   
   Comments: This is a shift-reduce style PCFG parser. Previous models are either stacking (first predict tag, then predict state of parse, and backpropogate to prase, and parse back-prop to source), or multi-tasking (which is very common in CV, predict tag and state at the same time). Stack-propogation allows back-propogate from state and tag both directly to the start. It's a solid good model, showing about 1% improvement to previous models.
   
## 2016 April

### Computer Vision

- **Attend, Infer, Repeat: Fast Scene Understanding with Generative Models**

   by Google Deep Mind, Geoffrey Hinton 
   
   Adobe Research
   
   Abstract: We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.
   
   link: [http://arxiv.org/pdf/1603.08575.pdf](http://arxiv.org/pdf/1603.08575.pdf)
   
   Comments: I haven't finished reading this paper, but this is a shiny example of combining the power of DL and PGM. Also Hinton uses RNN to predict how many objects in the image the model should attend to, that RNN generates a sequence of 1 until it generates a 0. This is using RNN to replace what a traditionally Chinese Restaurant Process or Indian Buffet Process would do.

- **Colorful Image Colorization**

   by Richard Zhang, Phillip Isola, Alexei A. Efros
      
   Abstract: Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and explore using class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward operation in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a "colorization Turing test", asking human subjects to choose between a generated and ground truth color image. Our method successfully fools humans 20% of the time, significantly higher than previous methods.
   
   link: [http://arxiv.org/pdf/1603.08511v1.pdf](http://arxiv.org/pdf/1603.08511v1.pdf)
   
   Comments: This paper is actually published in March. The colorization looks very realistic, and this paper is covered by TechCrunch.

- **Deep Networks with Stochastic Depth**

   by Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Weinberger
      
   Abstract: Very deep convolutional networks with hundreds or more layers have lead to significant reductions in error on competitive benchmarks like the ImageNet or COCO tasks. Although the unmatched expressiveness of the many deep layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes and the training time can be painfully slow even on modern computers. In this paper we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and obtain deep networks. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. The resulting networks are short (in expectation) during training and deep during testing. Training Residual Networks with stochastic depth is compellingly simple to implement, yet effective. We show that this approach successfully addresses the training difficulties of deep networks and complements the recent success of Residual and Highway Networks. It reduces training time substantially and improves the test errors on almost all data sets significantly (CIFAR-10, CIFAR-100, SVHN). Intriguingly, with stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91%) on CIFAR-10.
   
   link: [http://arxiv.org/pdf/1603.09382v2.pdf](http://arxiv.org/pdf/1603.09382v2.pdf)
   
   Comments: (Recommended by Socher)

### NLP

- **Recurrent Batch Normalization**

   by Tim Cooijmans, Nicolas Ballas, César Laurent, Çağlar Gülçehre, Aaron Courville
      
   Abstract: We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps. We evaluate our proposal on various sequential problems such as sequence classification, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and improved generalization.
   
   link: [http://arxiv.org/pdf/1603.09025v3.pdf](http://arxiv.org/pdf/1603.09025v3.pdf)
   
   Comments: (Recommended by Socher)

- **Recurrent Dropout without Memory Loss**

   by Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth
      
   Abstract: This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to forward connections of feed-forward architectures or RNNs, we propose to drop neurons directly in recurrent connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for the most popular recurrent networks: vanilla RNNs, Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks. Our experiments on three NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout.
   
   link: [http://arxiv.org/pdf/1603.05118v1.pdf](http://arxiv.org/pdf/1603.05118v1.pdf)
   
   Comments: This is one of my most favorite papers so far. It breaks down dropout, and RNN, and discuss what approach is correct and what is incorrect, in terms of retaining as much memory as possible, while harnessing the power of dropout generalization. It can serve as a guideline for many RNN techniques.
   
- ** A Corpus and Cloze Evaluation for Deeper Understanding of
Commonsense Stories ** 

   by Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen
      
   Abstract: Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.
   
   link: [http://arxiv.org/pdf/1604.01696v1.pdf](http://arxiv.org/pdf/1604.01696v1.pdf)
   
   Comments: This paper releases a new corpus on story understanding, which is a long-time problem/task in NLU/NLP. This paper also proposes a novel Cloze evaluation. Maybe a neural network can be developed for this kind of high-level understanding, proving neural network framework's ability to higher-level NLU tasks.
