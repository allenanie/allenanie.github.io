---
published: true
title: 2016 Edgy Paper Selection
layout: post
---


I decide to have a series of introductions for cutting edge papers. Those papaers are posted on Archive site like [arxiv.org](http://arxiv.org/). I recently subscribed to them. 

I will list those articles also based on themes, or relevancy, or (fancier) timelines. I won't discuss in great detail the content of the paper, but I'll leave my cursory comments.

## 2016 March

March is actually a great month, mostly due to the fact that ACL is finally wrapped up and we are seeing more papers being submitted. 

### Miscellaneous 

- **Topic Modeling Using Distributed Word Embeddings**

   by Ramandeep S Randhawa, Parag Jain, Gagan Madan
   
   University of Southern California, IIT Delhi
   
   Abstract: We propose a new algorithm for topic modeling, Vec2Topic, that identifies the main topics in a corpus using semantic information captured via high-dimensional distributed word embeddings. Our technique is unsupervised and generates a list of topics ranked with respect to importance. We find that it works better than existing topic modeling techniques such as Latent Dirichlet Allocation for identifying key topics in user-generated content, such as emails, chats, etc., where topics are diffused across the corpus. We also find that Vec2Topic works equally well for non-user generated content, such as papers, reports, etc., and for small corpora such as a single-document.
   
   link: [http://arxiv.org/abs/1603.04747](http://arxiv.org/abs/1603.04747)
   
   Comments: Might be interesting to check out, a replacement for LDA

### Computer Vision

- **Object Contour Detection with a Fully Convolutional Encoder-Decoder Network**

   by Jimei Yang, Brian Price, Scott Cohen, Honglak Lee, Ming-Hsuan Yang
   
   Adobe Research
   
   Abstract: We develop a deep learning algorithm for contour detection with a fully convolutional encoder-decoder network. Different from previous low-level edge detection, our algorithm focuses on detecting higher-level object contours. Our network is trained end-to-end on PASCAL VOC with refined ground truth from inaccurate polygon annotations, yielding much higher precision in object contour detection than previous methods. We find that the learned model generalizes well to unseen object classes from the same super-categories on MS COCO and can match state-of-the-art edge detection on BSDS500 with fine-tuning. By combining with the multiscale combinatorial grouping algorithm, our method can generate high-quality segmented object proposals, which significantly advance the state-of-the-art on PASCAL VOC (improving average recall from 0.62 to 0.67) with a relatively small amount of candidates (âˆ¼1660 per image).
   
   link: [http://arxiv.org/abs/1603.04530](http://arxiv.org/abs/1603.04530)
   
   Comments: We have seen RNN encoder-decoder network, but what about CNN encoder-decoder? This might provide some answer. The encoder part is a normal CNN to condense the spatial information, the decoder part is a "deconv" network.
   

### NLP

- **End-to-End Attention-based Large Vocabulary Speech Recognition**

   by Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, Yoshua Bengio
   
   University of Montreal, Bengio Group
   
   Abstract: Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.
   
   link: [http://arxiv.org/abs/1508.04395](http://arxiv.org/abs/1508.04395)
   
   Comments: This is a natural extension to what Bahdanau worked on in attention-based neural machine translation (he is the one who came up with attention: [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](http://arxiv.org/pdf/1409.0473v6.pdf)). Now they are applying the model to speech recognition.
   
   







   