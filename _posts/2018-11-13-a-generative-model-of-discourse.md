---
published: true
layout: post
draft: true
visible: 0
title: A Generative Model of Discourse
---
## A Generative Model of Discourse

Note: to people who are infinitely more well-versed in linguistics than I am, here I use "discourse" to indicate the meaning representation of a sentence. 

TLDR: This is a post based on Linear Algebraic Structure of Word Senses, with Applications to Polysemy ([link](https://arxiv.org/pdf/1601.03764.pdf)).

### Short Preface

There has been a lot interests on sentence-level representation learning. Similar to the explosion of word embedding, sentence embedding has captured a lot of attention with new papers being produced on ArXiv like grass sprouting in Spring. 

This article could easily be a lengthy linguistics paper, and such one is very needed for the current situation. Based on my very limited education in linguistics, I do not believe linguistis have an agreed upon set of "properties" that a sentence representation[^1] need to capture, nor do I believe adding more of such properties exhausitvely will finally have enough granularity to sift gold from sand -- find the most ideal sentence representation.

This post tries to take a closer look at the seminal work produced by Sanjeev Arora and his students, who took a principled approach to computationally infer the meaning representation from words, and then to sentences.

### A Generative Model of Sentence

<p style="text-align: center"><img src="https://github.com/windweller/windweller.github.io/blob/master/images/discourse-graph.png?raw=true" style="width:50%"> <br> <b> Figure 1 </b></p>

Arora et al. assume a discourse random vector[^2]  $c \sim \mathcal{N}(0, \Sigma)$ that represents the meaning of a sentence,  then every word of the sentence is generated by this random variable $c$. The probabilistic graphical representation is illustrated above. Then we can write the overall model as words in a window of size $n$ generated by $c$ as:

$$
P(w_1, w_2, ..., w_n | c) = \prod_{i=1}^n P(w_i | c)
$$

The probability of a word $w_i$ occur in the sequence can be described as a log-linear model:

$$
P(w_i | c) = \frac{\exp(c \cdot v_{w_i})}{Z_c}
$$

Note that $v_{w_i}​$ indicates the actual vector representation of $w_i​$, and this probability model uses dot-product distance to capture the probability of word $w_i​$ appear in the sequence, normalized by a partition function $Z_c = \sum_w \exp(v_w \cdot c)​$. This dictates that the discourse vector $c​$ will be close to words it produces in vector space.

Based on this very simple model, a very interesting theorem can be proved. Here, I write out the actual proof (with more details than the one provided in the paper, as well as an easy illustration on what's going on).

Theorem 1 of Arora et al. (2018)[^3] paper can be understood by introducing a vector $u$.  For a given word $w$ and its corresponding word vector $v_w$, we can compute a vector $u$. 

<p style="text-align: center"><img src="https://github.com/windweller/windweller.github.io/blob/master/images/discourse-theorem1-u.jpg?raw=true" style="width:35%"> <br> <b>Figure 2</b> </p>

For this word $w$, it must appear in different spans of words across the entire document. A random variable of a window of $n$ words can be introduced as $s$, a span. Computationally, the vector $u$ for the word $w​$ can be computed as follow:

$$
u = \frac{1}{k} \sum_{s \in \{s_1, ..., s_k\}} \frac{1}{n} \sum_{w_i \in s} v_{w_i}
$$

To even make this statement simpler, assume the above figure represents a tensor $S \in \mathcal{R}^{n \times k \times d}​$, we can easily run the following Numpy operation to obtain $u​$: `u = np.mean(np.mean(S, axis=0), axis=1)`. After knowing how $u​$ is computed, then we can understand Theorem 1:
$$
v_w = A u
$$

For any word, if we compute the corresponding vector $u$, the word embedding of this word can be obtained through a linear transformation (matrix multiplication) by a fixed matrix $A$. I provide some algebra walk through the proof of Theorem 1 in the paper. Readers who find it elementary or advanced can skip this block straight to the next section. 

(**Optional**)

The proof stands as long as the generative model in Figure 1 holds. We set to show that $\mathbb{E}(\frac{1}{n} \sum_{w_i \in s} v\_{w_i} \vert w \in s)$. 
$$
\mathbb{E}[c_s | w \in s] = \mathbb{E}[\mathbb{E}[c_s | s = w_1...w...w_n | w \in s]]
$$

This step is by "iterated expectation" or "law of total expectation". And the following step to show the pdf (probability density function) of $c \vert w$ is straightforward. The paper mentioned/set up the following equality that we can substitute: $Z\_c \approx Z \exp(\vert\vert c \vert\vert^2)$[^4], the probability density function of a multivariate normal distribution $c \sim (0, \Sigma)$ is $p(c) = \exp(-\frac{1}{2} c^T \Sigma^{-1}c)$, $\vert\vert c \vert\vert^2 = c^Tc = c^T I c$, and the log-linear model we assumed: $p(w \vert c) = \exp(c \cdot v\_w)$. Then the following steps are easy to see:

$$
\begin{align*}
p(c|w) &\propto p(w|c)p(c) \\
&\propto \frac{1}{Z} \exp(v_w \cdot c - c^T(\frac{1}{2} \Sigma^{-1} + I)c) \\
\end{align*}
$$

After obtaining the probability density function of $c \vert w​$, we can think about what kind of random variable this pdf suggests. Since there is a covariance matrix inverse $\Sigma^{-1}​$ invovled, we can try to re-arrange the terms to make it look more like a multivariate Gaussian distribution. Since we do want to know $\mathbb{E}(c \vert w)​$ (note this is for a specific word), we need to know what is the mean of this new distribution.

First, we ignore the covariance determinant term as it is a constant and in Arora's setting, the covariance matrix will be invertible -- "if the word vectors as random variables are isotropic to some extent, it will make the covariance matrix identifiable" (identifiable is equivalent to determinant not equal to 0). The assumption "isotropic word embedding" here means that word embedding dimensions should not be correlated with each other ($w \sim \mathcal{N}(0, \sigma I)$).

Then, all we need to do is to make $p(c \vert w)​$ appear in the form of $\exp(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu))​$. Since the form $c^T(\frac{1}{2} \Sigma^{-1} + I)c​$ looks very similar to the quadratic form that we need, we can let $B^{-1} = \frac{1}{2} \Sigma^{-1} + I​$ and let $B​$ be our new covariance matrix for $c \vert w​$. We can work out the equations from two side. We first assume $\mu​$ is the mean we want to solve:

$$
\begin{align*}
p(c|w) &\propto \exp(-\frac{1}{2} (c-\mu)^T B^{-1} (c-\mu)) \\
&= \exp(-\frac{1}{2}(c^T B^{-1} c - cB^{-1}\mu - \mu^TB^{-1}c + \mu^TB^{-1}\mu))\\
p(c|w) &\propto \frac{1}{Z} \exp(v_w \cdot c - c^T(\frac{1}{2} \Sigma^{-1} + I)c) \\
&= \frac{1}{Z} \exp(-\frac{1}{2}(-2 v_w \cdot c + c^TB^{-1}c))
\end{align*}
$$

Now we have two expressions of $p(c \vert w)​$. We can match the terms between two equations, one term already appears in both, but not $-2 v_w \cdot c​$. There are however two terms with negative signs in the top expansion. An algebraic trick that applies here is to just make them equal and hope things to work out -- we solve for $\mu​$:

$$
-2 v_w \cdot c = - cB^{-1}\mu - \mu^TB^{-1}c
$$

It is somewhat transparent that on the RHS (right hand side), $A$ needs to disappear since the LHS (left hand side) does not contain any $B$. To do that, $\mu$ should at least contain $A$ so that it cancels out with $B^{-1}$. Also the LHS has $v_w$ while RHS has none. Then the answer should be transparent: $\mu = Bv_w$. If you plug this in, the above equation holds, shows that this is our $\mu$. 

My stats PhD friend told me, if I saw a pdf in the form of $w^Tx - \frac{1}{2} x^TB^{-1}x$, then I can actually skip the above algebra and directly "see" this distribution of $x$ as mean $Bw$, with variance $B$. 

So now, we know that $c \vert w \sim \mathcal{N}(B^{-1}v_w, B)$ where $B = (\Sigma^{-1} + 2I)^{-1}$, the posterior distribution of $c$ after conditioning on a single word in the sequence. Thus  $\mathbb{E}(c \vert w) = (\Sigma^{-1} + 2I)^{-1} v_w$.

<p>Then we want to get the pdf that describes $c|w_1, ..., w_n​$. This part is relatively straightforward, no algebra trick / insight is required. The work mostly hinges on the following expression: </p>

$$
p(c|w_1, ..., w_n) \propto p(w_1,...,w_n|c) p(c) \propto p(c) \prod_{i=1}^n p(w_i|c) \\
= \frac{1}{Z^n} \exp(\sum_{i=1}^n v_{w_i}^Tc - \frac{1}{2} c^T(\Sigma^{-1} + 2nI)c)
$$

<p>The generation of words are independent with each other conditioned on $c​$. We already know the general expression of $p(w \vert c)​$. So the above equation evaluates to a form that we have already worked out above. We can skip the algebra and know that $\mathbb{E}[c \vert w_1, ..., w_n] \approx (\Sigma^{-1} + 2nI)^{-1} \sum_{i=1}^n v\_{w_i}​$.</p>

If you still recall the LHS and RHS of the law of total expectation (iterated expectation) expansion we did much earlier, then all that needs to be done to conclude the proof is to plug in the LHS and RHS based on the equations we derived. Feel free to refer to the paper since it offers a cleaner/shorter presentation.

$$
\mathbb{E}[c_s | w \in s] = \mathbb{E}[\mathbb{E}[c_s | s = w_1...w...w_n | w \in s]] \\
 (\Sigma^{-1} + 2I)^{-1} v_w \approx (\Sigma^{-1} + 2nI)^{-1} \sum_{i=1}^n v_{w_i}
$$

Therefore, we know that the matrix $A$ that we set out to find is now solvable by re-arranging the terms in above equations: $A = n(\Sigma^{-1} + 2I) (\Sigma^{-1} + 2nI)^{-1}$.

### Application to Word Senses

Intuitively, Theorem 1 dictates that a word has a **linear relationship** (fulfilled by matrix $A$) to the average of all the context vectors this word appears in: $v\_w = A \sum_s c_s$. This relationship is fully specified by $\Sigma$, the covariance of discourse random vector $c$. Theorem 2 of the paper can be interpreted as: $v\_w \approx \alpha v\_{s\_1} + \beta v\_{s\_2}$, a linear combination of 2 senses (each sense is represented as a vector). We can see the parallel between this linear decomposition and the transformed average of all context that word $w$ appears in.

### Relations to Language Modeling



### Implicit Solutions 

Unwittingly at first, Word2Vec is quickly shown to be an implicit solution to a non-convex co-occurence matrix decomposition. GloVE and other word embedding methods followed the lead and grounded these methods in theory. Are sentence embedding models, let it be InferSent, DisSent, ELMo, BERT, implicit solutions to a more principled discourse model? 

[^1]: In the scope of this post, we can assume it's an embedding. This is a very narrow interpretation that is ignoring decades of linguistic work on sentence representations. Interested readers can take a look at Kemp's Discourse Representation Theory framework.
[^2]: In most of Arora et al.'s work, "sentence meaning", "discourse", and "context" are used almost interchangeably. They all refer to a vector representation of a span of words, usually within a fixed window. 
[^3]:Linear Algebraic Structure of Word Senses, with Applications to Polysemy.
[^4]: This is proven in A Latent Variable Model Approach to PMI-based Word Embeddings, Lemma 2.1. They proved a concentration bound of this partition function under the Bayesian priors specified in the model of Figure 1. It seems to be a general bound linked to the self-normalizing property of log-linear models.

