---
published: true
title: 2016 Edgy Paper Selection
layout: post
---


I decide to have a series of introductions for cutting edge papers. Those papaers are posted on Archive site like [arxiv.org](http://arxiv.org/). I recently subscribed to them. 

I will list those articles also based on themes, or relevancy, or (fancier) timelines. I won't discuss in great detail the content of the paper, but I'll leave my cursory comments.

## 2016 March

March is actually a great month, mostly due to the fact that ACL is finally wrapped up and we are seeing more papers being submitted. 

### Miscellaneous 

- **Topic Modeling Using Distributed Word Embeddings**

   by Ramandeep S Randhawa, Parag Jain, Gagan Madan
   
   University of Southern California, IIT Delhi
   
   Abstract: We propose a new algorithm for topic modeling, Vec2Topic, that identifies the main topics in a corpus using semantic information captured via high-dimensional distributed word embeddings. Our technique is unsupervised and generates a list of topics ranked with respect to importance. We find that it works better than existing topic modeling techniques such as Latent Dirichlet Allocation for identifying key topics in user-generated content, such as emails, chats, etc., where topics are diffused across the corpus. We also find that Vec2Topic works equally well for non-user generated content, such as papers, reports, etc., and for small corpora such as a single-document.
   
   link: [http://arxiv.org/abs/1603.04747](http://arxiv.org/abs/1603.04747)
   
   Comments: Might be interesting to check out, a replacement for LDA
   
- **The Computational Power of Dynamic Bayesian Networks**

   by Joshua Brulé
   
   University of Maryland
   
   Abstract: This paper considers the computational power of constant size, dynamic Bayesian networks. Although discrete dynamic Bayesian networks are no more powerful than hidden Markov models, dynamic Bayesian networks with continuous random variables and discrete children of continuous parents are capable of performing Turing-complete computation. With modified versions of existing algorithms for belief propagation, such a simulation can be carried out in real time. This result suggests that dynamic Bayesian networks may be more powerful than previously considered. Relationships to causal models and recurrent neural networks are also discussed.
   
   link: [http://arxiv.org/abs/1603.06125](http://arxiv.org/abs/1603.06125)
   
   Comments: Dynamic Bayesian network is Turing-complete. Maybe it can give neural network a run for its money??

### Computer Vision

- **Object Contour Detection with a Fully Convolutional Encoder-Decoder Network**

   by Jimei Yang, Brian Price, Scott Cohen, Honglak Lee, Ming-Hsuan Yang
   
   Adobe Research
   
   Abstract: We develop a deep learning algorithm for contour detection with a fully convolutional encoder-decoder network. Different from previous low-level edge detection, our algorithm focuses on detecting higher-level object contours. Our network is trained end-to-end on PASCAL VOC with refined ground truth from inaccurate polygon annotations, yielding much higher precision in object contour detection than previous methods. We find that the learned model generalizes well to unseen object classes from the same super-categories on MS COCO and can match state-of-the-art edge detection on BSDS500 with fine-tuning. By combining with the multiscale combinatorial grouping algorithm, our method can generate high-quality segmented object proposals, which significantly advance the state-of-the-art on PASCAL VOC (improving average recall from 0.62 to 0.67) with a relatively small amount of candidates (∼1660 per image).
   
   link: [http://arxiv.org/abs/1603.04530](http://arxiv.org/abs/1603.04530)
   
   Comments: We have seen RNN encoder-decoder network, but what about CNN encoder-decoder? This might provide some answer. The encoder part is a normal CNN to condense the spatial information, the decoder part is a "deconv" network.
   

### NLP

- **End-to-End Attention-based Large Vocabulary Speech Recognition**

   by Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, Yoshua Bengio
   
   University of Montreal, Bengio Group
   
   Abstract: Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.
   
   link: [http://arxiv.org/abs/1508.04395](http://arxiv.org/abs/1508.04395)
   
   Comments: This is a natural extension to what Bahdanau worked on in attention-based neural machine translation (he is the one who came up with attention: [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](http://arxiv.org/pdf/1409.0473v6.pdf)). Now they are applying the model to speech recognition.
   
- **Tree-to-Sequence Attentional Neural Machine Translation**

   by Akiko Eriguchi, Kazuma Hashimoto, Yoshimasa Tsuruoka
   
   The University of Tokyo
   
   Abstract: Most of the existing neural machine translation (NMT) models focus on the conversion of sequential data and do not directly take syntax into consideration. We propose a novel end-to-end syntactic NMT model, extending a sequence-to-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT'15 English-to-Japanese dataset demonstrate that our proposed model outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system.
   
   link: [http://arxiv.org/abs/1603.06075](http://arxiv.org/abs/1603.06075)
   
   Comments: I think there are two camps of the deep learning NLP right now. One camp (mostly from the big three Deep Learning groups) is trying to be agnostic to linguistic knowledge. They are developing character-based models to learn language from ground up. Another camp is to try their best to incorporate linguistic structure or knowledge into deep learning framework. This paper is a beautiful example of the latter. It shows how people the proper way to bake syntax into neural network models.
   
- **How Transferable are Neural Networks in NLP Applications?**

   by Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, Zhi Jin
   
   Peking University
   
   Abstract: Transfer learning is aimed to make use of valuable knowledge in a source domain to help the model performance in a target domain. It is particularly important to neural networks because neural models are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct a series of empirical studies and provide an illuminating picture on the transferability of neural networks in NLP.

   
   link: [http://arxiv.org/abs/1603.06111](http://arxiv.org/abs/1603.06111)
   
   Comments: This article is building from the transfer learning literature in Computer Vision arena. I skipped the paper and read the conclusion and it turns out to be related to how similar two corpuses or two genres are. Seems to be a bit platitudinous. 






   