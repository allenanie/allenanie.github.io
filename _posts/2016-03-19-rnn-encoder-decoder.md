---
published: true
title: A Brief History of RNN Encoder Decoder
layout: post
---


RNN encoder-decoder has become the next heated field in Natural Language Processing, here is a history of its development. Papers are organized in a timely manner.

Ian Goodfellow's book has a great overall summary to it: http://www.deeplearningbook.org/contents/rnn.html
but sadly since it is a book, it lacks any recent development and any technical details.

Another interesting, more folksy way to approach is from WildML's recent post: http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/

RNN encoder-decoder network was started by Kyunghyun Cho ([Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation](http://arxiv.org/pdf/1406.1078v3.pdf)) and Ilya Sutskever ([Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)) both in 2014.

Sutskever's model constitutes end-to-end (sequence-to-sequence) training (with pure RNN), while Cho's model relies on scoring proposals generated by another machien translation system.

A huge drawback of these two initial models are they both use a fixed-length vector to represent the meaning of a source sentence. The dimension of this fixed-length vector can get too small to summarize a long sequence.

Bahdanau observed this phenomenon in his paper in 2015:, which became the second milestone paper for RNN encoder-decoder ([ NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](http://arxiv.org/pdf/1409.0473v6.pdf)).
